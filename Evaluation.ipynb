{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: CLAD anonymous authors\n",
    "Description: Script to evaluate the RawNet2, AASIST, Res-TSSDNet and CLAD model under manipulation attacks. Please check the configuration before testing.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml  # used by RawNet2 to read the configuration\n",
    "import json  # used to read config file\n",
    "import AASIST  # official AASIST implementation from https://github.com/clovaai/aasist/blob/main/models/AASIST.py\n",
    "import os\n",
    "import IPython.display as ipd  # used to display audio\n",
    "from tqdm import tqdm  # progress bar\n",
    "from Model import  DownStreamLinearClassifier, RawNetEncoderBaseline, RawNetBaseline, SSDNet1D, SAMOArgs  # SSDNet is the Res-TSSDNet Model\n",
    "from DatasetUtils import genSpoof_list, Dataset_ASVspoof2019_train  # ASVspoof dataset utils\n",
    "# Used to get the evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score\n",
    "from evaluate_tDCF_asvspoof19 import compute_eer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Manipulation classes used\n",
    "from DatasetUtils import VolumeChange, AddWhiteNoise, AddEnvironmentalNoise, WaveTimeStretch, AddEchoes, TimeShift, AddFade, ResampleAugmentation, pad_or_clip_batch\n",
    "import torchaudio.transforms\n",
    "from samo.samo.loss import SAMO\n",
    "from samo.samo.main import get_loader,init_params,update_embeds\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Configurations\n",
    "batch_size = 32\n",
    "gpu = 1  # GPU id to use\n",
    "torch.cuda.set_device(gpu)\n",
    "\n",
    "# Load Config file\n",
    "with open(\"./config.conf\", \"r\") as f_json:\n",
    "    config = json.loads(f_json.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name:str, config:dict):\n",
    "    if model_name == \"RawNet2\":\n",
    "        dir_yaml = config['rawnet2_config_path']\n",
    "        with open(dir_yaml, 'r') as f_yaml:\n",
    "            parser1 = yaml.safe_load(f_yaml)\n",
    "        rawnet2_model_path = config['rawnet2_model_path']\n",
    "        rawnet2_model = RawNetBaseline(parser1['model'], device)\n",
    "        rawnet2_model = rawnet2_model.to(device)\n",
    "        rawnet2_model.load_state_dict(torch.load(rawnet2_model_path,map_location=device))\n",
    "        print('RawNetBaseline model loaded : {}'.format(rawnet2_model_path))\n",
    "        nb_params = sum([param.view(-1).size()[0] for param in rawnet2_model.parameters()])\n",
    "        print(f\"Number of Rawnet2 params:{nb_params}\")\n",
    "        return rawnet2_model\n",
    "    if model_name == \"AASIST\":\n",
    "        with open(config['aasist_config_path'], \"r\") as f_json:\n",
    "            aasist_config = json.loads(f_json.read())\n",
    "        aasist_model_config = aasist_config[\"model_config\"]\n",
    "        aasist_model = AASIST.Model(aasist_model_config).to(device)\n",
    "        nb_params = sum([param.view(-1).size()[0] for param in aasist_model.parameters()])\n",
    "        print(\"Number of AASIST params:{}\".format(nb_params))\n",
    "        aasist_model.load_state_dict(\n",
    "            torch.load(config['aasist_model_path'], map_location=device))\n",
    "        print(\"Model loaded : {}\".format(config[\"aasist_model_path\"]))\n",
    "        return aasist_model\n",
    "    if model_name == \"ResTSSDNetModel\":\n",
    "        res_tssdnet_model = SSDNet1D()\n",
    "        check_point = torch.load(config['res_tssdnet_model_path'])\n",
    "        res_tssdnet_model.load_state_dict(check_point['model_state_dict'])\n",
    "        res_tssdnet_model = res_tssdnet_model.to(device)\n",
    "        return res_tssdnet_model\n",
    "    if model_name == \"SAMO\":\n",
    "        samo_model =torch.load(config['samo_model_path']).to(device)\n",
    "        return samo_model\n",
    "    if model_name == \"CLAD\":\n",
    "        with open(config['aasist_config_path'], \"r\") as f_json:        \n",
    "            aasist_config = json.loads(f_json.read())\n",
    "        aasist_model_config = aasist_config[\"model_config\"]\n",
    "        aasist_encoder = AASIST.AasistEncoder(aasist_model_config).to(device)\n",
    "        downstream_model = DownStreamLinearClassifier(aasist_encoder, input_depth=160)\n",
    "        checkpoint = torch.load(config['clad_model_path_for_evaluation'], map_location=device)\n",
    "        downstream_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        downstream_model = downstream_model.to(device)\n",
    "        return downstream_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics, if the threshold is not given, the threshold output by the EER calculation will be used.\n",
    "def get_eval_metrics(score_save_path, plot_figure=True, given_threshold=None, print_result=True):\n",
    "    cm_data = np.genfromtxt(score_save_path, dtype=str)\n",
    "    cm_keys = cm_data[:, 2]\n",
    "    # cm_keys = 'bonafide' means 1, 'spoof' means 0\n",
    "    cm_keys = np.where(cm_keys == 'bonafide', 1, 0)\n",
    "    cm_scores = cm_data[:, 3].astype(float)\n",
    "    # Compute EER\n",
    "    # Extract bona fide (real human) and spoof scores from the CM scores\n",
    "    bona_cm = cm_scores[cm_keys == 1]\n",
    "    spoof_cm = cm_scores[cm_keys == 0]\n",
    "    eer_cm, threshold = compute_eer(bona_cm, spoof_cm)\n",
    "\n",
    "    auc = roc_auc_score(cm_keys, cm_scores)\n",
    "    if given_threshold is not None:\n",
    "        threshold = given_threshold\n",
    "    y_pred = np.where(cm_scores > threshold, 1, 0)\n",
    "    f1 = f1_score(cm_keys, y_pred)\n",
    "    acc = balanced_accuracy_score(cm_keys, y_pred)\n",
    "    # compute False Acceptance Rate and False Rejection Rate\n",
    "    FAR = np.sum(cm_keys[y_pred == 1] == 0) / np.sum(cm_keys == 0)\n",
    "    FRR = np.sum(cm_keys[y_pred == 0] == 1) / np.sum(cm_keys == 1)\n",
    "    if print_result == True:\n",
    "        print(f\"EER:{eer_cm}, auc:{auc}, F1 score:{f1}, acc:{acc}, threshold:{threshold}, FAR:{FAR}, FRR:{FRR}\")\n",
    "    if plot_figure == True:\n",
    "        # ylgnbu_pal = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "        sns.histplot(bona_cm, kde=False, label='Real', stat=\"density\", element=\"step\", fill=False, bins='auto')\n",
    "        sns.histplot(spoof_cm, kde=False, label='Deepfake', stat=\"density\",element=\"step\", fill=False, bins='auto')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.xlabel('Prediction score')\n",
    "        plt.title('Prediction score histogram')\n",
    "    return (eer_cm, auc, f1, acc, threshold, FAR, FRR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation_19_LA_eval(model, score_save_path, model_name, database_path, augmentations=None, augmentations_on_cpu=None, batch_size = 1024, manipulation_on_real=True, cut_length = 64600):\n",
    "    # In asvspoof dataset, label = 1 means bonafide.\n",
    "    model.eval()\n",
    "    device = \"cuda\"\n",
    "    # load asvspoof 2019 LA eval dataset\n",
    "    d_label_trn, file_eval, utt2spk = genSpoof_list(dir_meta=database_path+\"ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\", is_train=False, is_eval=False)\n",
    "    print('no. of ASVspoof 2019 LA evaluating trials', len(file_eval))\n",
    "    asvspoof_LA_eval_dataset = Dataset_ASVspoof2019_train(list_IDs=file_eval, labels=d_label_trn, base_dir=os.path.join(\n",
    "        database_path+'ASVspoof2019_LA_eval/'), cut_length=cut_length, utt2spk=utt2spk)\n",
    "    asvspoof_2019_LA_eval_dataloader = DataLoader(asvspoof_LA_eval_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=8, pin_memory=True)  # added num_workders param to speed up.\n",
    "    with open(score_save_path, 'w') as file:  # This creates an empty file or empties an existing file\n",
    "        pass\n",
    "    if model_name == \"SAMO\":\n",
    "        samo_args = SAMOArgs()\n",
    "        samo_args.path_to_database = config['database_path'][:-3]  # SAMO default path does not include \"/LA\"\n",
    "        samo_args.batch_size = batch_size\n",
    "        samo_args.target = False  # we use all the evaluation data, rather than only the target data\n",
    "        # print(samo_args)\n",
    "        _, _, eval_data_loader, train_bona_loader, _, eval_enroll_loader, _ = get_loader(samo_args)\n",
    "        samo = SAMO(samo_args.enc_dim, m_real=samo_args.m_real, m_fake=samo_args.m_fake, alpha=samo_args.alpha).to(device)\n",
    "        if samo_args.val_sp:\n",
    "            # define and update eval centers\n",
    "            eval_enroll = update_embeds(samo_args.device, model, eval_enroll_loader)\n",
    "        else:  # use training centers without eval enrollment\n",
    "            if samo_args.one_hot:\n",
    "                spklist = ['LA_00' + str(spk_id) for spk_id in range(79, 99)]\n",
    "                tmp_center = torch.eye(samo_args.enc_dim)[:20]\n",
    "                eval_enroll = dict(zip(spklist, tmp_center))\n",
    "            else:\n",
    "                eval_enroll = update_embeds(samo_args.device, model, train_bona_loader)\n",
    "        samo.center = torch.stack(list(eval_enroll.values()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (audio_input, spks, labels) in enumerate(tqdm(asvspoof_2019_LA_eval_dataloader)):\n",
    "            score_list = []  \n",
    "            # audio_input = torch.squeeze(audio_input)\n",
    "            audio_input = audio_input.squeeze(1)\n",
    "            if augmentations_on_cpu != None:\n",
    "                audio_input = augmentations_on_cpu(audio_input)\n",
    "            \n",
    "            audio_input = audio_input.to(device)\n",
    "\n",
    "            if augmentations != None:\n",
    "                if manipulation_on_real == False:\n",
    "                    # note that some manipulation will change the length of the audio, so we need to clip or pad it to the same length\n",
    "                    audio_length = audio_input.shape[-1]\n",
    "                    # only apply the augmentation on the spoofed audio, and pad or clip it to the same length\n",
    "                    audio_input[labels==0] = pad_or_clip_batch(augmentations(audio_input[labels==0]), audio_length, random_clip=False)\n",
    "\n",
    "                else:\n",
    "                    audio_input = augmentations(audio_input)  \n",
    "            # check the length of the audio, if it is not the same as the cut_length, then repeat or clip it to the same length\n",
    "            if audio_input.shape[-1] < cut_length:\n",
    "                audio_input = audio_input.repeat(1, int(cut_length/audio_input.shape[-1])+1)[:, :cut_length]\n",
    "            elif audio_input.shape[-1] > cut_length:\n",
    "                audio_input = audio_input[:, :cut_length]\n",
    "            \n",
    "            if model_name == \"ResTSSDNetModel\":\n",
    "                audio_input = audio_input.unsqueeze(1)  # pretrained ResTSSDNetModel takes 3D input(batch, channel, waveform), so we need to add a dimension for channels\n",
    "            batch_out = model(audio_input)\n",
    "            if model_name == \"AASIST\":\n",
    "                batch_out = batch_out[1]  # the AASIST model output last_hidden_state and score and we only need the score\n",
    "            # The ResTSSDNetModel output two scores, but the order seems to be reversed. So we need to reverse it back.\n",
    "            # The ResTSSDNetModel takes bonafide as 0, reference: https://github.com/ghua-ac/end-to-end-synthetic-speech-detection/blob/main/data.py#L89\n",
    "            elif model_name == \"ResTSSDNetModel\":  \n",
    "                batch_out = batch_out[:, [1,0]]\n",
    "            elif model_name == \"SAMO\":\n",
    "                if samo_args.target:  # loss calculation for target-only speakers\n",
    "                    _, score = samo(batch_out[0], labels, spks, eval_enroll, samo_args.val_sp)\n",
    "                else:\n",
    "                    _, score = samo.inference(batch_out[0], labels, spks, eval_enroll, samo_args.val_sp)\n",
    "            if model_name == \"mesonet_whisper_mfcc_finetuned\":\n",
    "                batch_score = (batch_out[:, 0]).data.cpu().numpy().ravel()\n",
    "            elif model_name == \"SAMO\":\n",
    "                batch_score = score.data.cpu().numpy().ravel()\n",
    "            else:\n",
    "                batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "            label_list = ['bonafide' if i==1 else 'spoof' for i in labels]\n",
    "            score_list.extend(batch_score.tolist())\n",
    "\n",
    "            with open(score_save_path, 'a+') as fh:\n",
    "                for label, cm_score in zip(label_list,score_list):\n",
    "                    fh.write('- - {} {}\\n'.format(label, cm_score))\n",
    "            fh.close()   \n",
    "        print('Scores saved to {}'.format(score_save_path))\n",
    "    return  get_eval_metrics(score_save_path=score_save_path, plot_figure=False)# TODO: return EER, AUC and other things when I implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}  # create a empty dict to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dataset_path = config['noise_dataset_path']\n",
    "manipulations = {\n",
    "    \"no_augmentation\": None,\n",
    "    \"volume_change_50\": torchaudio.transforms.Vol(gain=0.5,gain_type='amplitude'),\n",
    "    \"volume_change_10\": torchaudio.transforms.Vol(gain=0.1,gain_type='amplitude'),\n",
    "    \"white_noise_15\": AddWhiteNoise(max_snr_db = 15, min_snr_db=15),\n",
    "    \"white_noise_20\": AddWhiteNoise(max_snr_db = 20, min_snr_db=20),\n",
    "    \"white_noise_25\": AddWhiteNoise(max_snr_db = 25, min_snr_db=25),\n",
    "    \"env_noise_wind\": AddEnvironmentalNoise(max_snr_db=20, min_snr_db=20, device=\"cuda\", noise_category=\"wind\", noise_dataset_path=noise_dataset_path),\n",
    "    \"env_noise_footsteps\": AddEnvironmentalNoise(max_snr_db=20, min_snr_db=20, device=\"cuda\", noise_category=\"footsteps\", noise_dataset_path=noise_dataset_path),\n",
    "    \"env_noise_breathing\": AddEnvironmentalNoise(max_snr_db=20, min_snr_db=20, device=\"cuda\", noise_category=\"breathing\", noise_dataset_path=noise_dataset_path),\n",
    "    \"env_noise_coughing\": AddEnvironmentalNoise(max_snr_db=20, min_snr_db=20, device=\"cuda\", noise_category=\"coughing\", noise_dataset_path=noise_dataset_path),\n",
    "    \"env_noise_rain\": AddEnvironmentalNoise(max_snr_db=20, min_snr_db=20, device=\"cuda\", noise_category=\"rain\", noise_dataset_path=noise_dataset_path),\n",
    "    \"env_noise_clock_tick\": AddEnvironmentalNoise(max_snr_db=20, min_snr_db=20, device=\"cuda\", noise_category=\"clock_tick\", noise_dataset_path=noise_dataset_path),\n",
    "    \"env_noise_sneezing\": AddEnvironmentalNoise(max_snr_db=20, min_snr_db=20, device=\"cuda\", noise_category=\"sneezing\", noise_dataset_path=noise_dataset_path),\n",
    "    \"timestretch_110\": WaveTimeStretch(max_ratio=1.10, min_ratio=1.10, n_fft=128),\n",
    "    \"timestretch_105\": WaveTimeStretch(max_ratio=1.05, min_ratio=1.05, n_fft=128),\n",
    "    \"timestretch_095\": WaveTimeStretch(max_ratio=0.95, min_ratio=0.95, n_fft=128),\n",
    "    \"timestretch_090\": WaveTimeStretch(max_ratio=0.90, min_ratio=0.90, n_fft=128),\n",
    "    \"echoes_1000_02\": AddEchoes(max_delay=1000, max_strengh=0.2, min_delay=1000, min_strength=0.2),\n",
    "    \"echoes_1000_05\": AddEchoes(max_delay=1000, max_strengh=0.5, min_delay=1000, min_strength=0.5),\n",
    "    \"echoes_2000_05\": AddEchoes(max_delay=2000, max_strengh=0.5, min_delay=2000, min_strength=0.5),\n",
    "    \"time_shift_1600\": TimeShift(max_shift=1600, min_shift=1600),\n",
    "    \"time_shift_16000\": TimeShift(max_shift=16000, min_shift=16000),\n",
    "    \"time_shift_32000\": TimeShift(max_shift=32000, min_shift=32000),\n",
    "    \"fade_50_linear\": AddFade(max_fade_size=0.5,fade_shape='linear', fix_fade_size=True),\n",
    "    \"fade_30_linear\": AddFade(max_fade_size=0.3,fade_shape='linear', fix_fade_size=True),\n",
    "    \"fade_10_linear\": AddFade(max_fade_size=0.1,fade_shape='linear', fix_fade_size=True),\n",
    "    \"fade_50_exponential\": AddFade(max_fade_size=0.5,fade_shape='exponential', fix_fade_size=True),\n",
    "    \"fade_50_quarter_sine\": AddFade(max_fade_size=0.5,fade_shape='quarter_sine', fix_fade_size=True),\n",
    "    \"fade_50_half_sine\": AddFade(max_fade_size=0.5,fade_shape='half_sine', fix_fade_size=True),\n",
    "    \"fade_50_logarithmic\": AddFade(max_fade_size=0.5,fade_shape='logarithmic', fix_fade_size=True),\n",
    "    \"resample_15000\": ResampleAugmentation([15000], device=\"cuda\"),\n",
    "    \"resample_15500\": ResampleAugmentation([15500], device=\"cuda\"),\n",
    "    \"resample_16500\": ResampleAugmentation([16500], device=\"cuda\"),\n",
    "    \"resample_17000\": ResampleAugmentation([17000], device=\"cuda\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"RawNet2\", \"AASIST\",\"ResTSSDNetModel\",\"SAMO\",\"CLAD\"]:\n",
    "    if model_name == \"ResTSSDNetModel\":\n",
    "        cut_length = 96000\n",
    "    elif model_name == \"RawNet2\":\n",
    "        cut_length = 64600  # 64000 in paper. However we found it used 64600 in the implementation(https://github.com/asvspoof-challenge/2021/blob/main/LA/Baseline-RawNet2/data_utils.py) \n",
    "    else:\n",
    "        cut_length = 64600\n",
    "\n",
    "    model = load_model(model_name,config)\n",
    "    for (manipulation_name, manipulation) in manipulations.items():\n",
    "        filename_prefix = model_name\n",
    "        evaluation_results[manipulation_name] = evaluation_19_LA_eval(model=model, model_name=model_name, database_path=config['database_path'], batch_size = batch_size, augmentations=manipulation, score_save_path=f\"scores/{filename_prefix}_{manipulation_name}_eval_19_LA_score.txt\", cut_length=cut_length)\n",
    "        print(f\"--------{manipulation_name} finished.--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Results for model: RawNet2 -----\n",
      "no_augmentation: F1 score:81.08%, FAR:4.60%\n",
      "volume_change_50: F1 score:86.90%, FAR:2.78%\n",
      "volume_change_10: F1 score:37.16%, FAR:36.62%\n",
      "white_noise_15: F1 score:54.24%, FAR:18.00%\n",
      "white_noise_20: F1 score:67.42%, FAR:10.09%\n",
      "white_noise_25: F1 score:73.38%, FAR:7.44%\n",
      "env_noise_wind: F1 score:82.86%, FAR:4.02%\n",
      "env_noise_footsteps: F1 score:86.34%, FAR:2.95%\n",
      "env_noise_breathing: F1 score:83.74%, FAR:3.74%\n",
      "env_noise_coughing: F1 score:81.99%, FAR:4.30%\n",
      "env_noise_rain: F1 score:70.46%, FAR:8.68%\n",
      "env_noise_clock_tick: F1 score:72.88%, FAR:7.65%\n",
      "env_noise_sneezing: F1 score:84.94%, FAR:3.37%\n",
      "timestretch_110: F1 score:96.43%, FAR:0.28%\n",
      "timestretch_105: F1 score:95.78%, FAR:0.44%\n",
      "timestretch_095: F1 score:96.24%, FAR:0.33%\n",
      "timestretch_090: F1 score:96.95%, FAR:0.16%\n",
      "echoes_1000_02: F1 score:84.34%, FAR:3.55%\n",
      "echoes_1000_05: F1 score:91.62%, FAR:1.48%\n",
      "echoes_2000_05: F1 score:87.14%, FAR:2.71%\n",
      "time_shift_1600: F1 score:82.66%, FAR:4.08%\n",
      "time_shift_16000: F1 score:82.92%, FAR:3.99%\n",
      "time_shift_32000: F1 score:89.16%, FAR:2.14%\n",
      "fade_50_linear: F1 score:57.71%, FAR:15.57%\n",
      "fade_30_linear: F1 score:72.97%, FAR:7.61%\n",
      "fade_10_linear: F1 score:80.50%, FAR:4.79%\n",
      "fade_50_exponential: F1 score:44.30%, FAR:27.09%\n",
      "fade_50_quarter_sine: F1 score:72.31%, FAR:7.88%\n",
      "fade_50_half_sine: F1 score:41.52%, FAR:30.42%\n",
      "fade_50_logarithmic: F1 score:81.14%, FAR:4.58%\n",
      "resample_15000: F1 score:83.48%, FAR:3.82%\n",
      "resample_15500: F1 score:83.15%, FAR:3.92%\n",
      "resample_16500: F1 score:83.33%, FAR:3.87%\n",
      "resample_17000: F1 score:83.95%, FAR:3.67%\n",
      "-----Results for model: AASIST -----\n",
      "no_augmentation: F1 score:96.11%, FAR:0.83%\n",
      "volume_change_50: F1 score:93.51%, FAR:1.49%\n",
      "volume_change_10: F1 score:71.25%, FAR:9.12%\n",
      "white_noise_15: F1 score:99.30%, FAR:0.07%\n",
      "white_noise_20: F1 score:99.02%, FAR:0.13%\n",
      "white_noise_25: F1 score:97.89%, FAR:0.40%\n",
      "env_noise_wind: F1 score:99.26%, FAR:0.08%\n",
      "env_noise_footsteps: F1 score:98.45%, FAR:0.26%\n",
      "env_noise_breathing: F1 score:98.84%, FAR:0.17%\n",
      "env_noise_coughing: F1 score:99.15%, FAR:0.10%\n",
      "env_noise_rain: F1 score:99.14%, FAR:0.10%\n",
      "env_noise_clock_tick: F1 score:99.12%, FAR:0.11%\n",
      "env_noise_sneezing: F1 score:99.04%, FAR:0.13%\n",
      "timestretch_110: F1 score:98.89%, FAR:0.16%\n",
      "timestretch_105: F1 score:99.26%, FAR:0.08%\n",
      "timestretch_095: F1 score:99.43%, FAR:0.04%\n",
      "timestretch_090: F1 score:99.45%, FAR:0.03%\n",
      "echoes_1000_02: F1 score:98.59%, FAR:0.23%\n",
      "echoes_1000_05: F1 score:99.29%, FAR:0.07%\n",
      "echoes_2000_05: F1 score:99.16%, FAR:0.10%\n",
      "time_shift_1600: F1 score:97.05%, FAR:0.60%\n",
      "time_shift_16000: F1 score:96.45%, FAR:0.75%\n",
      "time_shift_32000: F1 score:97.31%, FAR:0.54%\n",
      "fade_50_linear: F1 score:70.13%, FAR:9.63%\n",
      "fade_30_linear: F1 score:86.08%, FAR:3.60%\n",
      "fade_10_linear: F1 score:95.24%, FAR:1.05%\n",
      "fade_50_exponential: F1 score:49.15%, FAR:23.53%\n",
      "fade_50_quarter_sine: F1 score:85.32%, FAR:3.83%\n",
      "fade_50_half_sine: F1 score:42.17%, FAR:31.22%\n",
      "fade_50_logarithmic: F1 score:94.51%, FAR:1.23%\n",
      "resample_15000: F1 score:97.07%, FAR:0.59%\n",
      "resample_15500: F1 score:97.03%, FAR:0.60%\n",
      "resample_16500: F1 score:95.55%, FAR:0.97%\n",
      "resample_17000: F1 score:94.43%, FAR:1.25%\n",
      "-----Results for model: ResTSSDNetModel -----\n",
      "no_augmentation: F1 score:92.57%, FAR:1.63%\n",
      "volume_change_50: F1 score:90.50%, FAR:2.19%\n",
      "volume_change_10: F1 score:68.74%, FAR:10.11%\n",
      "white_noise_15: F1 score:30.56%, FAR:51.28%\n",
      "white_noise_20: F1 score:36.49%, FAR:39.23%\n",
      "white_noise_25: F1 score:43.03%, FAR:29.80%\n",
      "env_noise_wind: F1 score:38.05%, FAR:36.68%\n",
      "env_noise_footsteps: F1 score:64.01%, FAR:12.55%\n",
      "env_noise_breathing: F1 score:84.69%, FAR:3.91%\n",
      "env_noise_coughing: F1 score:92.39%, FAR:1.68%\n",
      "env_noise_rain: F1 score:35.84%, FAR:40.37%\n",
      "env_noise_clock_tick: F1 score:63.74%, FAR:12.70%\n",
      "env_noise_sneezing: F1 score:88.19%, FAR:2.84%\n",
      "timestretch_110: F1 score:98.34%, FAR:0.19%\n",
      "timestretch_105: F1 score:98.31%, FAR:0.20%\n",
      "timestretch_095: F1 score:99.17%, FAR:0.00%\n",
      "timestretch_090: F1 score:99.17%, FAR:0.00%\n",
      "echoes_1000_02: F1 score:90.53%, FAR:2.18%\n",
      "echoes_1000_05: F1 score:93.88%, FAR:1.29%\n",
      "echoes_2000_05: F1 score:92.66%, FAR:1.61%\n",
      "time_shift_1600: F1 score:89.83%, FAR:2.38%\n",
      "time_shift_16000: F1 score:90.29%, FAR:2.25%\n",
      "time_shift_32000: F1 score:90.64%, FAR:2.15%\n",
      "fade_50_linear: F1 score:88.34%, FAR:2.80%\n",
      "fade_30_linear: F1 score:92.39%, FAR:1.68%\n",
      "fade_10_linear: F1 score:94.35%, FAR:1.17%\n",
      "fade_50_exponential: F1 score:81.62%, FAR:4.91%\n",
      "fade_50_quarter_sine: F1 score:91.82%, FAR:1.83%\n",
      "fade_50_half_sine: F1 score:65.87%, FAR:11.55%\n",
      "fade_50_logarithmic: F1 score:93.17%, FAR:1.47%\n",
      "resample_15000: F1 score:99.18%, FAR:0.00%\n",
      "resample_15500: F1 score:99.18%, FAR:0.00%\n",
      "resample_16500: F1 score:92.01%, FAR:1.78%\n",
      "resample_17000: F1 score:90.89%, FAR:2.08%\n",
      "-----Results for model: SAMO -----\n",
      "no_augmentation: F1 score:94.94%, FAR:1.09%\n",
      "volume_change_50: F1 score:87.56%, FAR:3.11%\n",
      "volume_change_10: F1 score:74.50%, FAR:7.67%\n",
      "white_noise_15: F1 score:98.51%, FAR:0.22%\n",
      "white_noise_20: F1 score:96.22%, FAR:0.77%\n",
      "white_noise_25: F1 score:93.74%, FAR:1.40%\n",
      "env_noise_wind: F1 score:97.83%, FAR:0.38%\n",
      "env_noise_footsteps: F1 score:98.19%, FAR:0.29%\n",
      "env_noise_breathing: F1 score:97.01%, FAR:0.58%\n",
      "env_noise_coughing: F1 score:98.94%, FAR:0.12%\n",
      "env_noise_rain: F1 score:96.98%, FAR:0.58%\n",
      "env_noise_clock_tick: F1 score:98.09%, FAR:0.32%\n",
      "env_noise_sneezing: F1 score:97.48%, FAR:0.46%\n",
      "timestretch_110: F1 score:99.44%, FAR:0.00%\n",
      "timestretch_105: F1 score:99.43%, FAR:0.01%\n",
      "timestretch_095: F1 score:99.45%, FAR:0.00%\n",
      "timestretch_090: F1 score:99.45%, FAR:0.00%\n",
      "echoes_1000_02: F1 score:98.78%, FAR:0.15%\n",
      "echoes_1000_05: F1 score:99.28%, FAR:0.04%\n",
      "echoes_2000_05: F1 score:99.17%, FAR:0.07%\n",
      "time_shift_1600: F1 score:96.56%, FAR:0.69%\n",
      "time_shift_16000: F1 score:91.89%, FAR:1.89%\n",
      "time_shift_32000: F1 score:93.05%, FAR:1.58%\n",
      "fade_50_linear: F1 score:56.17%, FAR:17.65%\n",
      "fade_30_linear: F1 score:68.51%, FAR:10.34%\n",
      "fade_10_linear: F1 score:90.89%, FAR:2.16%\n",
      "fade_50_exponential: F1 score:46.60%, FAR:25.98%\n",
      "fade_50_quarter_sine: F1 score:67.05%, FAR:11.07%\n",
      "fade_50_half_sine: F1 score:44.53%, FAR:28.24%\n",
      "fade_50_logarithmic: F1 score:83.01%, FAR:4.53%\n",
      "resample_15000: F1 score:97.84%, FAR:0.38%\n",
      "resample_15500: F1 score:97.64%, FAR:0.43%\n",
      "resample_16500: F1 score:94.55%, FAR:1.19%\n",
      "resample_17000: F1 score:93.16%, FAR:1.55%\n",
      "-----Results for model: CLAD -----\n",
      "no_augmentation: F1 score:94.82%, FAR:1.11%\n",
      "volume_change_50: F1 score:96.48%, FAR:0.70%\n",
      "volume_change_10: F1 score:99.17%, FAR:0.06%\n",
      "white_noise_15: F1 score:98.95%, FAR:0.11%\n",
      "white_noise_20: F1 score:97.26%, FAR:0.51%\n",
      "white_noise_25: F1 score:96.01%, FAR:0.82%\n",
      "env_noise_wind: F1 score:93.74%, FAR:1.39%\n",
      "env_noise_footsteps: F1 score:94.59%, FAR:1.17%\n",
      "env_noise_breathing: F1 score:96.56%, FAR:0.68%\n",
      "env_noise_coughing: F1 score:98.60%, FAR:0.20%\n",
      "env_noise_rain: F1 score:98.43%, FAR:0.23%\n",
      "env_noise_clock_tick: F1 score:94.68%, FAR:1.15%\n",
      "env_noise_sneezing: F1 score:95.25%, FAR:1.01%\n",
      "timestretch_110: F1 score:99.13%, FAR:0.07%\n",
      "timestretch_105: F1 score:98.91%, FAR:0.12%\n",
      "timestretch_095: F1 score:99.18%, FAR:0.06%\n",
      "timestretch_090: F1 score:99.32%, FAR:0.03%\n",
      "echoes_1000_02: F1 score:98.55%, FAR:0.21%\n",
      "echoes_1000_05: F1 score:99.32%, FAR:0.03%\n",
      "echoes_2000_05: F1 score:99.15%, FAR:0.07%\n",
      "time_shift_1600: F1 score:95.88%, FAR:0.85%\n",
      "time_shift_16000: F1 score:95.57%, FAR:0.93%\n",
      "time_shift_32000: F1 score:95.74%, FAR:0.89%\n",
      "fade_50_linear: F1 score:96.73%, FAR:0.64%\n",
      "fade_30_linear: F1 score:95.50%, FAR:0.95%\n",
      "fade_10_linear: F1 score:95.05%, FAR:1.06%\n",
      "fade_50_exponential: F1 score:97.32%, FAR:0.50%\n",
      "fade_50_quarter_sine: F1 score:95.88%, FAR:0.85%\n",
      "fade_50_half_sine: F1 score:96.07%, FAR:0.80%\n",
      "fade_50_logarithmic: F1 score:95.57%, FAR:0.93%\n",
      "resample_15000: F1 score:96.68%, FAR:0.65%\n",
      "resample_15500: F1 score:96.03%, FAR:0.81%\n",
      "resample_16500: F1 score:93.81%, FAR:1.37%\n",
      "resample_17000: F1 score:92.83%, FAR:1.63%\n"
     ]
    }
   ],
   "source": [
    "# Show Result of fix threshold(selected by EER)\n",
    "# The results of white noise injection may have a small difference as there are randomness in the white noise generation.\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in [\"RawNet2\", \"AASIST\",\"ResTSSDNetModel\",\"SAMO\",\"CLAD\"]:\n",
    "    results[model_name] = {}\n",
    "    # get EER threshold\n",
    "    print(\"-----Results for model:\", model_name, \"-----\")\n",
    "    results[model_name][\"no_augmentation\"] = get_eval_metrics(f\"scores/{model_name}_no_augmentation_eval_19_LA_score.txt\", plot_figure=False, print_result=False)\n",
    "    for manipulation_name in manipulations:\n",
    "        results[model_name][manipulation_name] = get_eval_metrics(f\"scores/{model_name}_{manipulation_name}_eval_19_LA_score.txt\", plot_figure=False, given_threshold=results[model_name][\"no_augmentation\"][4], print_result=False)\n",
    "        print(f\"{manipulation_name}: F1 score:{results[model_name][manipulation_name][2]:.2%}, FAR:{results[model_name][manipulation_name][5]:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
